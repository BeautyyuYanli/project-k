#!/usr/bin/env python3

import json
import re
import subprocess
import sys
from pathlib import Path
from typing import Annotated, Optional

import typer

app = typer.Typer(add_completion=False)


def _ensure_core_src_on_sys_path() -> None:
    """Make `core/src` importable when this script is run from a repo checkout."""

    for parent in Path(__file__).resolve().parents:
        core_src = parent / "core" / "src"
        if core_src.is_dir():
            sys.path.insert(0, str(core_src))
            return


_ensure_core_src_on_sys_path()

from k.agent.memory.folder import FolderMemoryStore
from k.agent.memory.paths import memory_records_root_from_config_base
from k.config import Config


def _infer_chat_id_from_channel(in_channel: str) -> str | None:
    parts = in_channel.split("/")
    for idx, part in enumerate(parts[:-1]):
        if part == "chat":
            return parts[idx + 1]
    return None


def _validate_channel_path(name: str, value: str) -> str:
    channel = value.strip()
    if not channel:
        raise ValueError(f"{name} must not be empty")
    if channel.startswith("/") or channel.endswith("/") or "//" in channel:
        raise ValueError(f"Invalid {name}: {value!r}")
    return channel


LineMatch = tuple[int, str]
FileMatches = list[tuple[Path, list[LineMatch]]]


def _parse_rg_lines_with_numbers(output: str) -> list[tuple[Path, int, str]]:
    """Parse ripgrep output lines in `path:line:match` form."""
    parsed: list[tuple[Path, int, str]] = []
    for raw in output.splitlines():
        if not raw:
            continue
        parts = raw.split(":", 2)
        if len(parts) != 3:
            continue
        path_s, line_s, text = parts
        try:
            line_no = int(line_s)
        except ValueError:
            continue
        parsed.append((Path(path_s), line_no, text))
    return parsed


def search_files_first_match_per_file(
    files: list[Path], pattern: str, n: int
) -> FileMatches:
    """Search files for `pattern`, keeping the last N files (sorted by path).

    For each returned file, only the first match (line number + line) is kept.
    """
    if not files:
        return []

    file_args = [str(f) for f in files]
    res = subprocess.run(
        [
            "rg",
            "--max-count",
            "1",
            "--with-filename",
            "--line-number",
            "--no-heading",
            pattern,
        ]
        + file_args,
        capture_output=True,
        text=True,
    )
    if res.returncode not in (0, 1):  # 1 = no matches, 2 = rg error
        return []
    grouped: dict[Path, list[LineMatch]] = {}
    for path, line_no, text in _parse_rg_lines_with_numbers(res.stdout):
        grouped[path] = [(line_no, text)]

    selected = sorted(grouped.items(), key=lambda x: x[0])[-n:]
    return selected


class _OutputWriter:
    def __init__(self, *, out_path: Path) -> None:
        self._out_path = out_path
        self._fh = None

    def __enter__(self) -> "_OutputWriter":
        self._out_path.parent.mkdir(parents=True, exist_ok=True)
        # Use exclusive create to avoid clobbering when multiple runs race.
        self._fh = self._out_path.open("x", encoding="utf-8")
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        if self._fh is not None:
            self._fh.close()
            self._fh = None

    def line(self, s: str) -> None:
        self._fh.write(s)
        self._fh.write("\n")
        print(s)


def _preference_candidates(
    *, in_channel: str, from_id: str | None
) -> list[tuple[Path, str]]:
    """Build preference-file lookup candidates in root-to-leaf channel order."""

    pref_root = Path.home() / ".kapybara" / "preferences"
    parts = in_channel.split("/")
    prefixes = ["/".join(parts[:i]) for i in range(1, len(parts) + 1)]

    paths: list[tuple[Path, str]] = []
    for prefix in prefixes:
        paths.append((pref_root / f"{prefix}.md", f"Preference ({prefix}.md):"))
        paths.append(
            (
                pref_root / prefix / "PREFERENCES.md",
                f"Preference ({prefix}/PREFERENCES.md):",
            )
        )

    # Keep by_user behavior as-is for Telegram route handling.
    if from_id:
        platform = parts[0]
        paths.append(
            (
                pref_root / platform / "by_user" / f"{from_id}.md",
                f"User-specific Preference (from_id: {from_id}):",
            )
        )

    return paths


@app.command()
def main(
    in_channel: Annotated[
        str,
        typer.Option(
            help=(
                "Exact current input in_channel. "
                "Do not replace with a broader parent prefix."
            )
        ),
    ],
    from_id: Annotated[
        Optional[str], typer.Option(help="Sender id (defaults to channel chat id)")
    ] = None,
    kw: Annotated[Optional[str], typer.Option(help="Regex for extra narrowing")] = None,
    n: Annotated[int, typer.Option(help="Lines kept per route")] = 6,
    out: Annotated[
        Optional[Path],
        typer.Option(
            help="Output TSV file path. Required. Use a unique (random) path under /tmp to avoid races.",
            show_default=False,
        ),
    ] = None,
):
    """Retrieve Telegram memory candidates scoped to the current input channel.

    Callers must pass the exact current-input `in_channel` string. For thread
    inputs, pass the full thread channel path instead of truncating to
    `telegram/chat/<chat_id>`.

    This script emits retrieval candidates and keeps legacy by-user preference
    output (`<platform>/by_user/<from_id>.md`) for compatibility.
    """
    try:
        in_channel = _validate_channel_path("in_channel", in_channel)
    except ValueError as e:
        typer.echo(f"Error: {e}", err=True)
        raise typer.Exit(code=2) from e

    if from_id is None:
        from_id = _infer_chat_id_from_channel(in_channel)

    if out is None:
        typer.echo("Error: missing required option '--out'.", err=True)
        typer.echo(
            "Tip: use a unique file to avoid races, e.g. --out /tmp/tg_ctx_<unique>.tsv",
            err=True,
        )
        raise typer.Exit(code=2)

    out_path = out.expanduser()
    records_root = memory_records_root_from_config_base(Config().config_base)
    store = FolderMemoryStore(records_root.parent)
    channel_root = in_channel.split("/", 1)[0]

    try:
        with _OutputWriter(out_path=out_path) as w:
            detailed_files = store.filter_by_in_channel(in_channel_prefix=in_channel)
            # The user route intentionally searches across the same channel root
            # (e.g. all `telegram/*` records), not only the current in_channel
            # subtree. This keeps per-user recall available in thread channels.
            user_scope_files = store.filter_by_in_channel(
                in_channel_prefix=channel_root
            )
            matches_channel: FileMatches = [
                (path, []) for path in sorted(detailed_files)[-n:]
            ]

            matches_user: FileMatches = []
            if from_id is not None:
                # Regex matching escaped quotes in detailed.jsonl lines.
                from_re = rf'\\"from\\"\s*:\s*\{{\s*\\"id\\"\s*:\s*{from_id}'
                matches_user = search_files_first_match_per_file(
                    user_scope_files, from_re, n
                )

            matches_kw: FileMatches = []
            if kw:
                matches_kw = store.search_by_keywords(
                    files=detailed_files,
                    pattern=kw,
                    n=n,
                )

            # Collect results
            results: dict[str, dict[str, object]] = {}

            def add_match(match_list: FileMatches, route_name: str) -> None:
                for path, line_matches in match_list:
                    # Map detailed.jsonl back to core.json
                    core_path = path.parent / (
                        path.name.removesuffix(".detailed.jsonl") + ".core.json"
                    )
                    if not core_path.exists():
                        continue

                    mem_id = core_path.name.removesuffix(".core.json")
                    if mem_id not in results:
                        results[mem_id] = {
                            "routes": {route_name},
                            "core_json": core_path.read_text().strip(),
                            "matched_detailed_lines": [],
                        }
                    else:
                        routes = results[mem_id].get("routes")
                        if isinstance(routes, set):
                            routes.add(route_name)

                    # Only the kw route is treated as a "content match" worth
                    # surfacing in the main TSV output; channel/user routes are
                    # candidate finding.
                    if route_name == "kw" and line_matches:
                        results[mem_id]["matched_detailed_lines"] = [
                            {"line": line_no, "text": text}
                            for line_no, text in line_matches
                        ]

            add_match(matches_channel, "channel")
            add_match(matches_user, "user")
            add_match(matches_kw, "kw")

            # Keep by_user preference output for compatibility while channel
            # prefix preferences are now injected by agent system prompts.
            if from_id is not None:
                for path, title in _preference_candidates(
                    in_channel=in_channel, from_id=from_id
                ):
                    if "by_user" not in path.parts:
                        continue
                    if path.exists():
                        for line in f"{title}\n{path.read_text()}\n---".splitlines():
                            w.line(line)

            # Output TSV
            w.line("# id\troutes\tcore_json\tmatched_detailed_lines")
            for mem_id in sorted(results.keys()):
                data = results[mem_id]
                routes_obj = data.get("routes", set())
                routes = routes_obj if isinstance(routes_obj, set) else set()
                routes_str = ",".join(sorted(str(route) for route in routes))
                matched = json.dumps(
                    data.get("matched_detailed_lines", []), ensure_ascii=False
                )
                core_json = str(data.get("core_json", ""))
                w.line(f"{mem_id}\t{routes_str}\t{core_json}\t{matched}")
    except FileExistsError:
        typer.echo(f"Error: output file already exists: {out_path}", err=True)
        typer.echo(
            "Tip: use a unique file to avoid races, e.g. --out /tmp/tg_ctx_<unique>.tsv",
            err=True,
        )
        raise typer.Exit(code=1)

    typer.echo(
        f"Output saved to: {out_path} (channel={in_channel}, candidates: {len(results)})",
        err=True,
    )


if __name__ == "__main__":
    app()
