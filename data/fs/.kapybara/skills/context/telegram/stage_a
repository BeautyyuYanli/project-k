#!/usr/bin/env -S uv run --script --quiet
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "typer",
# ]
# ///

import json
import re
import subprocess
from pathlib import Path
from typing import Optional, Annotated

import typer

app = typer.Typer(add_completion=False)


def _channel_has_prefix(*, channel: str, prefix: str) -> bool:
    return channel == prefix or channel.startswith(prefix + "/")


def _record_in_channel(payload: object) -> str | None:
    if not isinstance(payload, dict):
        return None

    in_channel = payload.get("in_channel")
    if isinstance(in_channel, str):
        return in_channel

    # Backward compatibility for legacy records.
    legacy_kind = payload.get("kind")
    if isinstance(legacy_kind, str):
        return legacy_kind
    return None


def _infer_chat_id_from_channel(in_channel: str) -> str | None:
    parts = in_channel.split("/")
    for idx, part in enumerate(parts[:-1]):
        if part == "chat":
            return parts[idx + 1]
    return None


def _validate_channel_path(name: str, value: str) -> str:
    channel = value.strip()
    if not channel:
        raise ValueError(f"{name} must not be empty")
    if channel.startswith("/") or channel.endswith("/") or "//" in channel:
        raise ValueError(f"Invalid {name}: {value!r}")
    return channel


def get_detailed_files(root: Path, *, in_channel_prefix: str) -> list[Path]:
    """Find detailed files whose core record `in_channel` matches a prefix."""

    root_segment = in_channel_prefix.split("/", 1)[0]
    root_pattern = re.escape(root_segment)
    grep_pattern = rf'"(?:in_channel|kind)"\s*:\s*"{root_pattern}(?:/|")'

    try:
        res = subprocess.run(
            [
                "rg",
                "-l",
                "--sort",
                "path",
                "-g",
                "*.core.json",
                grep_pattern,
                str(root),
            ],
            capture_output=True,
            text=True,
        )
    except OSError:
        return []

    if res.returncode not in (0, 1):
        return []

    core_files = [line for line in res.stdout.splitlines() if line]
    detailed_files = []
    for cf in core_files:
        core_path = Path(cf)
        try:
            payload = json.loads(core_path.read_text(encoding="utf-8"))
        except (OSError, ValueError):
            continue

        record_channel = _record_in_channel(payload)
        if record_channel is None:
            continue
        if not _channel_has_prefix(channel=record_channel, prefix=in_channel_prefix):
            continue

        df = core_path.parent / (
            core_path.name.removesuffix(".core.json") + ".detailed.jsonl"
        )
        if df.exists():
            detailed_files.append(df)
    return detailed_files


LineMatch = tuple[int, str]
FileMatches = list[tuple[Path, list[LineMatch]]]


def _parse_rg_lines_with_numbers(output: str) -> list[tuple[Path, int, str]]:
    """Parse ripgrep output lines in `path:line:match` form."""
    parsed: list[tuple[Path, int, str]] = []
    for raw in output.splitlines():
        if not raw:
            continue
        parts = raw.split(":", 2)
        if len(parts) != 3:
            continue
        path_s, line_s, text = parts
        try:
            line_no = int(line_s)
        except ValueError:
            continue
        parsed.append((Path(path_s), line_no, text))
    return parsed

def search_files_first_match_per_file(files: list[Path], pattern: str, n: int) -> FileMatches:
    """Search files for `pattern`, keeping the last N files (sorted by path).

    For each returned file, only the first match (line number + line) is kept.
    """
    if not files:
        return []
    
    file_args = [str(f) for f in files]
    res = subprocess.run(
        [
            "rg",
            "--max-count",
            "1",
            "--with-filename",
            "--line-number",
            "--no-heading",
            pattern,
        ]
        + file_args,
        capture_output=True,
        text=True,
    )
    if res.returncode not in (0, 1):  # 1 = no matches, 2 = rg error
        return []
    grouped: dict[Path, list[LineMatch]] = {}
    for path, line_no, text in _parse_rg_lines_with_numbers(res.stdout):
        grouped[path] = [(line_no, text)]

    selected = sorted(grouped.items(), key=lambda x: x[0])[-n:]
    return selected

def search_files_all_matches_grouped(files: list[Path], pattern: str, n: int) -> FileMatches:
    """Search files for `pattern`, keeping the last N files (sorted by path).

    For each returned file, up to the last N matched lines are kept (so the kw
    route can report multiple matches within the same memory record).
    """
    if not files:
        return []

    file_args = [str(f) for f in files]
    res = subprocess.run(
        ["rg", "--with-filename", "--line-number", "--no-heading", pattern] + file_args,
        capture_output=True,
        text=True,
    )
    if res.returncode not in (0, 1):  # 1 = no matches, 2 = rg error
        return []
    grouped: dict[Path, list[LineMatch]] = {}
    for path, line_no, text in _parse_rg_lines_with_numbers(res.stdout):
        grouped.setdefault(path, []).append((line_no, text))

    selected_paths = sorted(grouped.keys())[-n:]
    selected: FileMatches = []
    for path in selected_paths:
        selected.append((path, grouped[path][-n:]))
    return selected


class _OutputWriter:
    def __init__(self, *, out_path: Path) -> None:
        self._out_path = out_path
        self._fh = None

    def __enter__(self) -> "_OutputWriter":
        self._out_path.parent.mkdir(parents=True, exist_ok=True)
        # Use exclusive create to avoid clobbering when multiple runs race.
        self._fh = self._out_path.open("x", encoding="utf-8")
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        if self._fh is not None:
            self._fh.close()
            self._fh = None

    def line(self, s: str) -> None:
        self._fh.write(s)
        self._fh.write("\n")
        print(s)


def _preference_candidates(
    *, in_channel: str, from_id: str | None
) -> list[tuple[Path, str]]:
    """Build preference lookup candidates in root-to-leaf channel order."""

    pref_root = Path.home() / "preferences"
    parts = in_channel.split("/")
    prefixes = ["/".join(parts[:i]) for i in range(1, len(parts) + 1)]

    paths: list[tuple[Path, str]] = []
    for prefix in prefixes:
        paths.append((pref_root / f"{prefix}.md", f"Preference ({prefix}.md):"))
        paths.append(
            (
                pref_root / prefix / "PREFERENCES.md",
                f"Preference ({prefix}/PREFERENCES.md):",
            )
        )

    # Keep by_user behavior as-is for now.
    if from_id:
        platform = parts[0]
        paths.append(
            (
                pref_root / platform / "by_user" / f"{from_id}.md",
                f"User-specific Preference (from_id: {from_id}):",
            )
        )

    return paths


@app.command()
def main(
    in_channel: Annotated[
        str, typer.Option(help="Input channel prefix, e.g. telegram/chat/123")
    ],
    from_id: Annotated[
        Optional[str], typer.Option(help="Sender id (defaults to channel chat id)")
    ] = None,
    kw: Annotated[Optional[str], typer.Option(help="Regex for extra narrowing")] = None,
    root: Annotated[Path, typer.Option(help="Memories root")] = Path.home() / "memories/records",
    n: Annotated[int, typer.Option(help="Lines kept per route")] = 6,
    out: Annotated[
        Optional[Path],
        typer.Option(
            help="Output TSV file path. Required. Use a unique (random) path under /tmp to avoid races.",
            show_default=False,
        ),
    ] = None,
):
    try:
        in_channel = _validate_channel_path("in_channel", in_channel)
    except ValueError as e:
        typer.echo(f"Error: {e}", err=True)
        raise typer.Exit(code=2) from e

    if from_id is None:
        from_id = _infer_chat_id_from_channel(in_channel)

    if out is None:
        typer.echo("Error: missing required option '--out'.", err=True)
        typer.echo(
            "Tip: use a unique file to avoid races, e.g. --out /tmp/tg_ctx_<unique>.tsv",
            err=True,
        )
        raise typer.Exit(code=2)

    out_path = out.expanduser()

    try:
        with _OutputWriter(out_path=out_path) as w:
            detailed_files = get_detailed_files(root, in_channel_prefix=in_channel)
            matches_channel: FileMatches = [
                (path, []) for path in sorted(detailed_files)[-n:]
            ]

            matches_user: FileMatches = []
            if from_id is not None:
                # Regex matching escaped quotes in detailed.jsonl lines.
                from_re = rf'\\"from\\"\s*:\s*\{{\s*\\"id\\"\s*:\s*{from_id}'
                matches_user = search_files_first_match_per_file(
                    detailed_files, from_re, n
                )

            matches_kw = []
            if kw:
                matches_kw = search_files_all_matches_grouped(detailed_files, kw, n)

            # Collect results
            results = {} # id -> {routes: set, core_json: str, matched_detailed_lines: list[dict]}
            
            def add_match(match_list: FileMatches, route_name: str) -> None:
                for path, line_matches in match_list:
                    # Map detailed.jsonl back to core.json
                    core_path = path.parent / (path.name.removesuffix(".detailed.jsonl") + ".core.json")
                    if not core_path.exists(): continue
                    
                    mem_id = core_path.name.removesuffix(".core.json")
                    if mem_id not in results:
                        results[mem_id] = {
                            "routes": {route_name},
                            "core_json": core_path.read_text().strip(),
                            "matched_detailed_lines": []
                        }
                    else:
                        results[mem_id]["routes"].add(route_name)

                    # Only the kw route is treated as a "content match" worth surfacing
                    # in the main TSV output; channel/user routes are candidate finding.
                    if route_name == "kw" and line_matches:
                        results[mem_id]["matched_detailed_lines"] = [
                            {"line": line_no, "text": text} for line_no, text in line_matches
                        ]

            add_match(matches_channel, "channel")
            add_match(matches_user, "user")
            add_match(matches_kw, "kw")

            # Preferences
            prefs = []
            for path, title in _preference_candidates(
                in_channel=in_channel, from_id=from_id
            ):
                if path.exists():
                    prefs.append(f"{title}\n{path.read_text()}\n---")

            if prefs:
                for block in "\n".join(prefs).splitlines():
                    w.line(block)

            # Output TSV
            w.line("# id\troutes\tcore_json\tmatched_detailed_lines")
            for mem_id in sorted(results.keys()):
                data = results[mem_id]
                routes_str = ",".join(sorted(data["routes"]))
                matched = json.dumps(data["matched_detailed_lines"], ensure_ascii=False)
                w.line(f"{mem_id}\t{routes_str}\t{data['core_json']}\t{matched}")
    except FileExistsError:
        typer.echo(f"Error: output file already exists: {out_path}", err=True)
        typer.echo(
            "Tip: use a unique file to avoid races, e.g. --out /tmp/tg_ctx_<unique>.tsv",
            err=True,
        )
        raise typer.Exit(code=1)

    typer.echo(
        f"Output saved to: {out_path} (channel={in_channel}, candidates: {len(results)})",
        err=True,
    )

if __name__ == "__main__":
    app()
