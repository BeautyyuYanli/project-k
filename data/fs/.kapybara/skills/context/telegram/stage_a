#!/usr/bin/env -S uv run --script --quiet
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "typer",
# ]
# ///

import subprocess
import json
import tempfile
from pathlib import Path
from typing import Optional, Annotated
import typer

app = typer.Typer(add_completion=False)

def get_detailed_files(root: Path) -> list[Path]:
    """Find all detailed.jsonl files that correspond to telegram memories."""
    try:
        res = subprocess.run(
            ["rg", "-l", "--sort", "path", "-g", "*.core.json", r'"kind"\s*:\s*"telegram"', str(root)],
            capture_output=True, text=True, check=True
        )
        core_files = res.stdout.strip().split('\n')
    except subprocess.CalledProcessError:
        return []

    detailed_files = []
    for cf in core_files:
        if not cf: continue
        # cf is like /path/to/id.core.json
        df = Path(cf).parent / (Path(cf).name.removesuffix(".core.json") + ".detailed.jsonl")
        if df.exists():
            detailed_files.append(df)
    return detailed_files

LineMatch = tuple[int, str]
FileMatches = list[tuple[Path, list[LineMatch]]]

def _parse_rg_lines_with_numbers(output: str) -> list[tuple[Path, int, str]]:
    """Parse ripgrep output lines in `path:line:match` form."""
    parsed: list[tuple[Path, int, str]] = []
    for raw in output.splitlines():
        if not raw:
            continue
        parts = raw.split(":", 2)
        if len(parts) != 3:
            continue
        path_s, line_s, text = parts
        try:
            line_no = int(line_s)
        except ValueError:
            continue
        parsed.append((Path(path_s), line_no, text))
    return parsed

def search_files_first_match_per_file(files: list[Path], pattern: str, n: int) -> FileMatches:
    """Search files for `pattern`, keeping the last N files (sorted by path).

    For each returned file, only the first match (line number + line) is kept.
    """
    if not files:
        return []
    
    file_args = [str(f) for f in files]
    res = subprocess.run(
        [
            "rg",
            "--max-count",
            "1",
            "--with-filename",
            "--line-number",
            "--no-heading",
            pattern,
        ]
        + file_args,
        capture_output=True,
        text=True,
    )
    if res.returncode not in (0, 1):  # 1 = no matches, 2 = rg error
        return []
    grouped: dict[Path, list[LineMatch]] = {}
    for path, line_no, text in _parse_rg_lines_with_numbers(res.stdout):
        grouped[path] = [(line_no, text)]

    selected = sorted(grouped.items(), key=lambda x: x[0])[-n:]
    return selected

def search_files_all_matches_grouped(files: list[Path], pattern: str, n: int) -> FileMatches:
    """Search files for `pattern`, keeping the last N files (sorted by path).

    For each returned file, up to the last N matched lines are kept (so the kw
    route can report multiple matches within the same memory record).
    """
    if not files:
        return []

    file_args = [str(f) for f in files]
    res = subprocess.run(
        ["rg", "--with-filename", "--line-number", "--no-heading", pattern] + file_args,
        capture_output=True,
        text=True,
    )
    if res.returncode not in (0, 1):  # 1 = no matches, 2 = rg error
        return []
    grouped: dict[Path, list[LineMatch]] = {}
    for path, line_no, text in _parse_rg_lines_with_numbers(res.stdout):
        grouped.setdefault(path, []).append((line_no, text))

    selected_paths = sorted(grouped.keys())[-n:]
    selected: FileMatches = []
    for path in selected_paths:
        selected.append((path, grouped[path][-n:]))
    return selected


class _OutputWriter:
    def __init__(self, *, out_path: Path) -> None:
        self._out_path = out_path
        self._fh = None

    def __enter__(self) -> "_OutputWriter":
        self._out_path.parent.mkdir(parents=True, exist_ok=True)
        # Use exclusive create to avoid clobbering when multiple runs race.
        self._fh = self._out_path.open("x", encoding="utf-8")
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        if self._fh is not None:
            self._fh.close()
            self._fh = None

    def line(self, s: str) -> None:
        self._fh.write(s)
        self._fh.write("\n")
        print(s)


@app.command()
def main(
    chat_id: Annotated[str, typer.Option(help="Telegram chat.id")],
    from_id: Annotated[Optional[str], typer.Option(help="Sender id (defaults to chat-id)")] = None,
    kw: Annotated[Optional[str], typer.Option(help="Regex for extra narrowing")] = None,
    root: Annotated[Path, typer.Option(help="Memories root")] = Path.home() / "memories/records",
    n: Annotated[int, typer.Option(help="Lines kept per route")] = 6,
    out: Annotated[
        Optional[Path],
        typer.Option(
            help="Output TSV file path. Required. Use a unique (random) path under /tmp to avoid races.",
            show_default=False,
        ),
    ] = None,
):
    if from_id is None:
        from_id = chat_id

    if out is None:
        typer.echo("Error: missing required option '--out'.", err=True)
        typer.echo(
            "Tip: use a unique file to avoid races, e.g. --out /tmp/tg_ctx_<unique>.tsv",
            err=True,
        )
        raise typer.Exit(code=2)

    out_path = out.expanduser()

    try:
        with _OutputWriter(out_path=out_path) as w:
            detailed_files = get_detailed_files(root)
            
            # Regexes (matching escaped quotes in detailed.jsonl)
            chat_re = rf'\\"chat\\"\s*:\s*\{{\s*\\"id\\"\s*:\s*{chat_id}'
            from_re = rf'\\"from\\"\s*:\s*\{{\s*\\"id\\"\s*:\s*{from_id}'
            
            matches_chat = search_files_first_match_per_file(detailed_files, chat_re, n)
            matches_user = search_files_first_match_per_file(detailed_files, from_re, n)
            
            matches_kw = []
            if kw:
                # Find all files matching chat_re first
                try:
                    res = subprocess.run(
                        ["rg", "-l", "--sort", "path", chat_re] + [str(f) for f in detailed_files],
                        capture_output=True, text=True
                    )
                    if res.returncode not in (0, 1):  # 1 = no matches, 2 = rg error
                        raise RuntimeError(f"rg failed for chat filter: rc={res.returncode}")
                    all_chat_files = [Path(p) for p in res.stdout.strip().split('\n') if p]
                    if all_chat_files:
                        matches_kw = search_files_all_matches_grouped(all_chat_files, kw, n)
                except Exception:
                    pass

            # Collect results
            results = {} # id -> {routes: set, core_json: str, matched_detailed_lines: list[dict]}
            
            def add_match(match_list: FileMatches, route_name: str) -> None:
                for path, line_matches in match_list:
                    # Map detailed.jsonl back to core.json
                    core_path = path.parent / (path.name.removesuffix(".detailed.jsonl") + ".core.json")
                    if not core_path.exists(): continue
                    
                    mem_id = core_path.name.removesuffix(".core.json")
                    if mem_id not in results:
                        results[mem_id] = {
                            "routes": {route_name},
                            "core_json": core_path.read_text().strip(),
                            "matched_detailed_lines": []
                        }
                    else:
                        results[mem_id]["routes"].add(route_name)

                    # Only the kw route is treated as a "content match" worth surfacing
                    # in the main TSV output; chat/user routes are just for candidate finding.
                    if route_name == "kw" and line_matches:
                        results[mem_id]["matched_detailed_lines"] = [
                            {"line": line_no, "text": text} for line_no, text in line_matches
                        ]

            add_match(matches_chat, "chat")
            add_match(matches_user, "user")
            add_match(matches_kw, "kw")

            # Preferences
            kind = "telegram"
            prefs = []
            pref_paths = [
                (Path.home() / f"preferences/{kind}/preferences.md", f"Global Preference ({kind}):"),
                (Path.home() / f"preferences/{kind}/by_chat/{chat_id}.md", f"Chat-specific Preference (chat_id: {chat_id}):"),
                (Path.home() / f"preferences/{kind}/by_user/{from_id}.md", f"User-specific Preference (from_id: {from_id}):"),
            ]
            
            for path, title in pref_paths:
                if path.exists():
                    prefs.append(f"{title}\n{path.read_text()}\n---")
            
            if prefs:
                for block in "\n".join(prefs).splitlines():
                    w.line(block)

            # Output TSV
            w.line("# id\troutes\tcore_json\tmatched_detailed_lines")
            for mem_id in sorted(results.keys()):
                data = results[mem_id]
                routes_str = ",".join(sorted(data["routes"]))
                matched = json.dumps(data["matched_detailed_lines"], ensure_ascii=False)
                w.line(f"{mem_id}\t{routes_str}\t{data['core_json']}\t{matched}")
    except FileExistsError:
        typer.echo(f"Error: output file already exists: {out_path}", err=True)
        typer.echo(
            "Tip: use a unique file to avoid races, e.g. --out /tmp/tg_ctx_<unique>.tsv",
            err=True,
        )
        raise typer.Exit(code=1)

    typer.echo(f"Output saved to: {out_path} (candidates: {len(results)})", err=True)

if __name__ == "__main__":
    app()
